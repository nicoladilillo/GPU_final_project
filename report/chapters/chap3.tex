\chapter{Implementation}
\label{chap3}

\section{CPU}

Before to proceed with the parallelization of this algorithm, a sequential version, executable on CPU, has been written.
At the end the results will be the same and the only different will be the time of computation.

This is the prototype of final release, more easy to develop, especially for the final
part that compare the best scheduling latency among all. Here memory optimization are not so useful for better performance, 
while algorithm improvement can speed up the computation.

Two versions are implementation:

\begin{itemize}
    \item Version 1.0, create single combination that care about all repetitions, more efficient;
    \item Version 2.0, create single combination that care about single repetition, less efficient but more 
    similar to what has been implemented for GPU;
\end{itemize}

\section{GPU Data Structure}

To perform the GPU elaboration data structure has been changed in order to lighten the overall process of memory copy,
the real bottleneck of this approach. For this reason the device has to reduce the data before to pass them to host.

The main big variables are all related about node and operation.
Both keep track of information that are not useful for scheduling computation and so, before to call GPU kernel,
they are filtered to maintain only the main data usable for computation, the useless ones will be always available on the host.

To improve the elaboration also other size variable are been accurately choose to use as less memory as possible.

\section{Scheduling Algorithm}

The section \ref{list_scheduling} explains which algorithm has been choose to performer the scheduling operation. 
This part is identical both for GPU and CPU versions, change only the implementation of some variables used, 
that are always the same.
How will be explained in the next chapter those variables, to be more precise those arrays,
are one of critical points that have been improved to achieve better performance.


\section{GPU Kernel}

Different version has been created to see which one executes the fastest computation.

By the way all of them use the same mechanism to analysed which repeated combination has the best latency.
In the same block all threads save the result of latency and their own repeated combination on shared memory so,
when all schedules terminates, the best set of resources is picked from the same block and pass to host trough
the main memory copy operation. At the end CPU checks all results coming from kernel and choose the best one with minimum latency,
in case of equality the one with minimum area is chosen. This part is implemented almost in the same way in all versions.

\subsection{Version 1}

This is the basic version where each thread handle all repetition, given a combination. 
If not all operations are covered the thread doesn't create the repetition and goes on. The scheduling of single repetition
is executed only if the area constraint is respected.

Combinations are group 1024 at a time, each using a single block inside a stream, using a max number of stream. At the same time
it is possible to have block that work with different value of k ($\binom{n}{k}$) thanks to the stream.

All the arrays are allocated on shared memory in a dynamic way, respecting the max amount of shared memory for block.

For long repetitions time problems occurs and the kernel stops its execution.

\subsection{Version 2}

Each thread handle a single repeat combination and, also if the combination don't cover all the needed operation or
if area constraint are not respected, the thread is created but the scheduling is not lunch.
Different versions have been created to test which is the best way to arrange variable declaration.

\begin{itemize}
    \item Version 2.0, like the previous one but with the new property of single repetition for thread.
    
    \item Version 2.1, use less shared memory that is compensated by dynamic allocation when arrays are used only inside a 
    single block.
    
    \item Version 2.2, not use anymore dynamic allocation and improve the usage of resources,
    allocating space only for the ones used in scheduling.
    
    \item Version 2.3, instead of shared memory, arrays of fixed dimension are used inside each thread when possible and the 
    global memory is called only when latency value obtain is better than previous one. Furthermore now, at the end execution of 
    each blocks inside a thread, only thread with id zero take care to choose which has the best latency. 
\end{itemize}

In al the versions the variables that take information about nodes, operations and best latency thread are keep on shared memory 
to allow start thread to keep the best result and the end.

From this versions is pretty evident that dynamic allocation is a technique that needed a lot of time and
not the best choice while, the usage of local register of version 2.3 and call global memory as lest as possible, 
are big improvement.

Also here combinations are group 1024 at a time, each using a single block inside a stream.

\subsection{Version 3}

Based on version 2.3, in previous version each stream handles single block while in this version streams are not more used 
and each kernel call works with all blocks that, given the combination $\binom{n}{k}$, has the same value of $k$.
Now much more global memory is instantiate at the same time and this could result in an import speed up of the system that
call global memory copy only and organize the overall workload in a better way.

\subsection{Version 4}

Merge the advantage of multi streams of version 2.3 and with the possibility to work with more blocks inside each stream of
version 3. Then a further improved of version 4.1 has been add in order to have the possibility to have workload that stress
the GPU with the creation of billion of threads.
Furthermore, the number of heave computation operations has been reduced as much as possible.